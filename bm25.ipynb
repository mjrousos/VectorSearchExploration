{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BM25\n",
    "\n",
    "[BM25 (aka Okapi BM25)](https://en.wikipedia.org/wiki/Okapi_BM25) is an optimized version of TF-IDF. \"BM\" stands for \"Best Matching\". With TF-IDF, scores increase linearly with the frequency of a word. BM-25 is a ranking function that adjusts TF-IDF scores by decaying the impact of higher frequencies. It is useful because more instances don't necessarily mean linearly more relevance.\n",
    "\n",
    "With TF-IDF, doubling the frequency of a word will double the TF-IDF score. However, doubling the frequency of a word with BM25 will increase the score by a smaller amount (at least at larger frequencies)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BM25 Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def bm25(term, doc, corpus, corpus_length = -1, average_doc_length = -1, k = 1.25, b = 0.75):\n",
    "    term = term.lower()\n",
    "    if corpus_length == -1:\n",
    "     corpus_length = len(corpus)\n",
    "    if average_doc_length == -1:\n",
    "     average_doc_length = sum([len(d) for d in corpus]) / corpus_length\n",
    "    frequency = doc.count(term)\n",
    "\n",
    "    if frequency == 0:\n",
    "     return 0\n",
    "\n",
    "    tf = (frequency * (k + 1)) / \\\n",
    "         (frequency + k * (1 - b + b * len(doc) / average_doc_length))\n",
    "\n",
    "    docs_with_term = sum([1 for doc in corpus if term in doc])\n",
    "    \n",
    "    idf = np.log(((corpus_length - docs_with_term + 0.5) / \\\n",
    "                  (docs_with_term + 0.5)) \\\n",
    "                 + 1)\n",
    "    return tf * idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corpus of documents to search\n",
    "import string\n",
    "import os\n",
    "\n",
    "directory = \".\\plays\"\n",
    "files = {}\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        with open(file_path, \"r\") as file:\n",
    "            file_name = os.path.splitext(filename)[0]\n",
    "            file_contents = file.read().lower().translate(str.maketrans('', '', string.punctuation)).split()\n",
    "            files[file_name] = file_contents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Romeo: 6.554749288102218\n",
      "Aristotle: 0.0\n",
      "Mercutio: 6.479393819617477\n",
      "Poison: 0.6396043966805856\n"
     ]
    }
   ],
   "source": [
    "print(f'Romeo: {bm25(\"Romeo\", files[\"Romeo and Juliet\"], files.values())}')\n",
    "print(f'Aristotle: {bm25(\"Aristotle\", files[\"Romeo and Juliet\"], files.values())}')\n",
    "print(f'Mercutio: {bm25(\"Mercutio\", files[\"Romeo and Juliet\"], files.values())}')\n",
    "print(f'Poison: {bm25(\"Poison\", files[\"Romeo and Juliet\"], files.values())}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown above, BM25 can directly determine the relevance of specific terms to a document. It's also possible to vectorize text using BM25 so that documents can be compared for similarity. This is useful for information retrieval tasks based on larger input queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23443"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def bm25_vectorizer(doc, vocab, corpus, corpus_length = -1, average_doc_length = -1, k = 1.25, b = 0.75):\n",
    "    ret = []\n",
    "    for word in vocab:\n",
    "        ret.append(bm25(word, doc, corpus, corpus_length, average_doc_length, k, b))\n",
    "    return ret\n",
    "\n",
    "full_vocab = set([term for doc in files.values() for term in doc])\n",
    "len(full_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, vectorizing large documents is slow. This takes over 12 minutes on my Surface Book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizing A Midsummer Night's Dream... 0/27\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizing All's Well That Ends Well... 1/27\n",
      "Vectorizing Antony and Cleopatra... 2/27\n",
      "Vectorizing As You Like It... 3/27\n",
      "Vectorizing Cymbeline... 4/27\n",
      "Vectorizing King Lear... 5/27\n",
      "Vectorizing Loves Labours Lost... 6/27\n",
      "Vectorizing Measure for Measure... 7/27\n",
      "Vectorizing Much Ado About Nothing... 8/27\n",
      "Vectorizing Othello the Moore of Venice... 9/27\n",
      "Vectorizing Pericles Prince of Tyre... 10/27\n",
      "Vectorizing Romeo and Juliet... 11/27\n",
      "Vectorizing The Comedy of Errors... 12/27\n",
      "Vectorizing The Life and Death of Julius Caesar... 13/27\n",
      "Vectorizing The Merchant of Venice... 14/27\n",
      "Vectorizing The Merry Wives of Windsor... 15/27\n",
      "Vectorizing The Taming of the Shrew... 16/27\n",
      "Vectorizing The Tempest... 17/27\n",
      "Vectorizing The Tragedy of Coriolanus... 18/27\n",
      "Vectorizing The Tragedy of Hamlet Prince of Denmark... 19/27\n",
      "Vectorizing The Tragedy of Macbeth... 20/27\n",
      "Vectorizing Timon of Athens... 21/27\n",
      "Vectorizing Titus Andronicus... 22/27\n",
      "Vectorizing Troilus and Cressida... 23/27\n",
      "Vectorizing Twelfth Night... 24/27\n",
      "Vectorizing Two Gentlemen of Verona... 25/27\n",
      "Vectorizing Winter's Tale... 26/27\n"
     ]
    }
   ],
   "source": [
    "vectors = {}\n",
    "corpus_length = len(files)\n",
    "average_doc_length = sum([len(d) for d in files.values()]) / corpus_length\n",
    "for file in files:\n",
    "    print(f'Vectorizing {file}... {len(vectors)}/{len(files)}')\n",
    "    vectors[file] = bm25_vectorizer(files[file], full_vocab, files.values(), corpus_length=corpus_length, average_doc_length=average_doc_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity (vector1, vector2):\n",
    "    dot_product = np.dot(vector1, vector2)\n",
    "    norm_vector1 = np.linalg.norm(vector1)\n",
    "    norm_vector2 = np.linalg.norm(vector2)\n",
    "    return dot_product / (norm_vector1 * norm_vector2)\n",
    "\n",
    "corpus_length = len(files)\n",
    "average_doc_length = sum([len(d) for d in files.values()]) / corpus_length\n",
    "def check_document_relevance(query):\n",
    "    class Relevance:\n",
    "        def __init__(self, name, similarity):\n",
    "            self.name = name\n",
    "            self.similarity = similarity\n",
    "\n",
    "    query_tokens = query.lower().translate(str.maketrans('', '', string.punctuation)).split()\n",
    "    query_vector = bm25_vectorizer(query_tokens, full_vocab, files.values(), corpus_length=corpus_length, average_doc_length=average_doc_length)\n",
    "    print(f'Relevance for \"{query}\":')\n",
    "    print('--------------')\n",
    "    top_answers = [Relevance(name, cosine_similarity(query_vector, vectors[name])) for name in vectors]\n",
    "    top_answers.sort(key=lambda x: x.similarity, reverse=True)\n",
    "    for answer in top_answers[:5]:\n",
    "        print(f'{answer.name}: {answer.similarity}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relevance for \"Which play has three witches?\":\n",
      "--------------\n",
      "The Comedy of Errors: 0.046227095367736955\n",
      "The Tragedy of Macbeth: 0.045913790816210893\n",
      "Twelfth Night: 0.0003240812381666201\n",
      "All's Well That Ends Well: 0.0003175582201290583\n",
      "Timon of Athens: 0.0003049081655583579\n",
      "\n",
      "Relevance for \"Which play has a friar as an important character?\":\n",
      "--------------\n",
      "Much Ado About Nothing: 0.03270930487018254\n",
      "Measure for Measure: 0.032399563849089555\n",
      "Romeo and Juliet: 0.021691265120807567\n",
      "Two Gentlemen of Verona: 0.02063497593366803\n",
      "All's Well That Ends Well: 0.01723354767858958\n",
      "\n",
      "Relevance for \"Which play has a character named Caesar?\":\n",
      "--------------\n",
      "Measure for Measure: 0.021260249293138158\n",
      "Cymbeline: 0.018468163575386397\n",
      "The Life and Death of Julius Caesar: 0.017032372573670268\n",
      "Twelfth Night: 0.015455845460253321\n",
      "The Tragedy of Hamlet Prince of Denmark: 0.015327980459987666\n",
      "\n",
      "Relevance for \"Caesar?\":\n",
      "--------------\n",
      "The Life and Death of Julius Caesar: 0.0274850129556844\n",
      "Antony and Cleopatra: 0.022945441522943914\n",
      "Cymbeline: 0.02084590144831791\n",
      "Measure for Measure: 0.015989544762324948\n",
      "The Tragedy of Hamlet Prince of Denmark: 0.012310695513687069\n",
      "\n",
      "Relevance for \"Which play is set in Denmark?\":\n",
      "--------------\n",
      "The Tragedy of Hamlet Prince of Denmark: 0.055740735624583274\n",
      "Two Gentlemen of Verona: 3.3655202297815085e-05\n",
      "Twelfth Night: 3.292141195357881e-05\n",
      "As You Like It: 3.217270950609134e-05\n",
      "The Merchant of Venice: 3.161991147435006e-05\n",
      "\n"
     ]
    }
   ],
   "source": [
    "check_document_relevance(\"Which play has three witches?\")\n",
    "check_document_relevance(\"Which play has a friar as an important character?\")\n",
    "check_document_relevance(\"Which play has a character named Caesar?\")\n",
    "check_document_relevance(\"Caesar?\")\n",
    "check_document_relevance(\"Which play is set in Denmark?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems to actually perform worse for \"sentence similarity\". It's possible that reducing the impact of very common terms is causing the model to miss the most important terms in the search query (and allowing them to be swamped by less important terms). TF-IDF appears better at determining relevance for a question with a lot of \"filler\" words whereas BM25 is better at determining relevance for a question with fewer, more important words. Adjusting the K and B parameters in BM25 could help with this."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

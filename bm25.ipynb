{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BM25\n",
    "\n",
    "[BM25 (aka Okapi BM25)](https://en.wikipedia.org/wiki/Okapi_BM25) is an optimized version of TF-IDF. \"BM\" stands for \"Best Matching\". With TF-IDF, scores increase linearly with the frequency of a word. BM-25 is a ranking function that adjusts TF-IDF scores by decaying the impact of higher frequencies. It is useful because more instances don't necessarily mean linearly more relevance.\n",
    "\n",
    "With TF-IDF, doubling the frequency of a word will double the TF-IDF score. However, doubling the frequency of a word with BM25 will increase the score by a smaller amount (at least at larger frequencies)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BM25 Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def bm25(term, doc, corpus, corpus_length = -1, average_doc_length = -1, k = 1.25, b = 0.75):\n",
    "    term = term.lower()\n",
    "    if corpus_length == -1:\n",
    "     corpus_length = len(corpus)\n",
    "    if average_doc_length == -1:\n",
    "     average_doc_length = sum([len(d) for d in corpus]) / corpus_length\n",
    "    frequency = doc.count(term)\n",
    "\n",
    "    if frequency == 0:\n",
    "     return 0\n",
    "\n",
    "    tf = (frequency * (k + 1)) / \\\n",
    "         (frequency + k * (1 - b + b * len(doc) / average_doc_length))\n",
    "\n",
    "    docs_with_term = sum([1 for doc in corpus if term in doc])\n",
    "    \n",
    "    idf = np.log(((corpus_length - docs_with_term + 0.5) / \\\n",
    "                  (docs_with_term + 0.5)) \\\n",
    "                 + 1)\n",
    "    return tf * idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corpus of documents to search\n",
    "import string\n",
    "import os\n",
    "\n",
    "directory = \".\\plays\"\n",
    "files = {}\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        with open(file_path, \"r\") as file:\n",
    "            file_name = os.path.splitext(filename)[0]\n",
    "            file_contents = file.read().lower().translate(str.maketrans('', '', string.punctuation)).split()\n",
    "            files[file_name] = file_contents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Romeo: {bm25(\"Romeo\", files[\"Romeo and Juliet\"], files.values())}')\n",
    "print(f'Aristotle: {bm25(\"Aristotle\", files[\"Romeo and Juliet\"], files.values())}')\n",
    "print(f'Mercutio: {bm25(\"Mercutio\", files[\"Romeo and Juliet\"], files.values())}')\n",
    "print(f'Poison: {bm25(\"Poison\", files[\"Romeo and Juliet\"], files.values())}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown above, BM25 can directly determine the relevance of specific terms to a document. It's also possible to vectorize text using BM25 so that documents can be compared for similarity. This is useful for information retrieval tasks based on larger input queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bm25_vectorizer(doc, vocab, corpus, corpus_length = -1, average_doc_length = -1, k = 1.25, b = 0.75):\n",
    "    ret = []\n",
    "    for word in vocab:\n",
    "        ret.append(bm25(word, doc, corpus, corpus_length, average_doc_length, k, b))\n",
    "    return ret\n",
    "\n",
    "full_vocab = set([term for doc in files.values() for term in doc])\n",
    "len(full_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, vectorizing large documents is slow. This takes over 12 minutes on my Surface Book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = {}\n",
    "corpus_length = len(files)\n",
    "average_doc_length = sum([len(d) for d in files.values()]) / corpus_length\n",
    "for file in files:\n",
    "    print(f'Vectorizing {file}... {len(vectors)}/{len(files)}')\n",
    "    vectors[file] = bm25_vectorizer(files[file], full_vocab, files.values(), corpus_length=corpus_length, average_doc_length=average_doc_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity (vector1, vector2):\n",
    "    dot_product = np.dot(vector1, vector2)\n",
    "    norm_vector1 = np.linalg.norm(vector1)\n",
    "    norm_vector2 = np.linalg.norm(vector2)\n",
    "    return dot_product / (norm_vector1 * norm_vector2)\n",
    "\n",
    "corpus_length = len(files)\n",
    "average_doc_length = sum([len(d) for d in files.values()]) / corpus_length\n",
    "def check_document_relevance(query):\n",
    "    class Relevance:\n",
    "        def __init__(self, name, similarity):\n",
    "            self.name = name\n",
    "            self.similarity = similarity\n",
    "\n",
    "    query_tokens = query.lower().translate(str.maketrans('', '', string.punctuation)).split()\n",
    "    query_vector = bm25_vectorizer(query_tokens, full_vocab, files.values(), corpus_length=corpus_length, average_doc_length=average_doc_length)\n",
    "    print(f'Relevance for \"{query}\":')\n",
    "    print('--------------')\n",
    "    top_answers = [Relevance(name, cosine_similarity(query_vector, vectors[name])) for name in vectors]\n",
    "    top_answers.sort(key=lambda x: x.similarity, reverse=True)\n",
    "    for answer in top_answers[:5]:\n",
    "        print(f'{answer.name}: {answer.similarity}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_document_relevance(\"Which play has three witches?\")\n",
    "check_document_relevance(\"Which play has a friar as an important character?\")\n",
    "check_document_relevance(\"Which play has a character named Caesar?\")\n",
    "check_document_relevance(\"Caesar?\")\n",
    "check_document_relevance(\"Which play is set in Denmark?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems to actually perform worse for \"sentence similarity\". It's possible that reducing the impact of very common terms is causing the model to miss the most important terms in the search query (and allowing them to be swamped by less important terms). TF-IDF appears better at determining relevance for a question with a lot of \"filler\" words whereas BM25 is better at determining relevance for a question with fewer, more important words. Adjusting the K and B parameters in BM25 could help with this."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

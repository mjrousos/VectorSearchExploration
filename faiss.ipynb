{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Facebook AI Similarity Search\n",
    "\n",
    "Comparing two vectors is easy. Cosine similarity is a simple and effective technique. But what if you have thousands or even millions of vectors? How do you find the most similar vectors to a given query vector? Time required to compare (via cosine similarity) with all vectors scales linearly, so comparing to a hundred vectors is not a problem but even our simple searches through plays in the previous workbook took ~200ms to run. On a much larger corpus this would take seconds or tens of seconds for a search. This is the problem that similarity search solves. Running cosine similarity over such a large set of vectors is not feasible, so we need to use other approaches. This is what [Facebook AI Similarity Search (FAISS)](https://ai.meta.com/tools/faiss/) does. It uses indexing to speed up the search process.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\src\\mjrousos\\VectorSearchExploration\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Available models: https://www.sbert.net/docs/pretrained_models.html\n",
    "model = SentenceTransformer('multi-qa-mpnet-base-dot-v1')\n",
    "# model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "# model = SentenceTransformer('multi-qa-distilbert-cos-v1')\n",
    "# model = SentenceTransformer('sentence-transformers/all-roberta-large-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corpus of documents to search\n",
    "import os\n",
    "\n",
    "directory = \".\\plays\"\n",
    "files = {}\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        with open(file_path, \"r\") as file:\n",
    "            file_name = os.path.splitext(filename)[0]\n",
    "            file_contents = file.read()\n",
    "            files[file_name] = file_contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For FAISS exploration, let's use a small chunk size so that we get many vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunking plays...\n",
      "Embedding chunks...\n",
      "Complete\n",
      "A Midsummer Night's Dream: 488 chunks\n",
      "All's Well That Ends Well: 644 chunks\n",
      "Antony and Cleopatra: 806 chunks\n",
      "As You Like It: 599 chunks\n",
      "Cymbeline: 803 chunks\n",
      "King Lear: 797 chunks\n",
      "Loves Labours Lost: 640 chunks\n",
      "Measure for Measure: 631 chunks\n",
      "Much Ado About Nothing: 598 chunks\n",
      "Othello the Moore of Venice: 783 chunks\n",
      "Pericles Prince of Tyre: 539 chunks\n",
      "Romeo and Juliet: 724 chunks\n",
      "The Comedy of Errors: 446 chunks\n",
      "The Life and Death of Julius Caesar: 580 chunks\n",
      "The Merchant of Venice: 582 chunks\n",
      "The Merry Wives of Windsor: 692 chunks\n",
      "The Taming of the Shrew: 627 chunks\n",
      "The Tempest: 488 chunks\n",
      "The Tragedy of Coriolanus: 820 chunks\n",
      "The Tragedy of Hamlet Prince of Denmark: 893 chunks\n",
      "The Tragedy of Macbeth: 523 chunks\n",
      "Timon of Athens: 550 chunks\n",
      "Titus Andronicus: 610 chunks\n",
      "Troilus and Cressida: 806 chunks\n",
      "Twelfth Night: 600 chunks\n",
      "Two Gentlemen of Verona: 491 chunks\n",
      "Winter's Tale: 682 chunks\n"
     ]
    }
   ],
   "source": [
    "import semchunk\n",
    "import tiktoken\n",
    "\n",
    "class Chunk:\n",
    "    def __init__(self, text, doc_name, embedding):\n",
    "        self.text = text\n",
    "        self.doc_name = doc_name\n",
    "        self.embedding = embedding\n",
    "\n",
    "# Chunk the files into smaller portions\n",
    "corpus_chunks = {}\n",
    "doc_names = list(files.keys())\n",
    "encoder = tiktoken.encoding_for_model('gpt-4')\n",
    "\n",
    "print(\"Chunking plays...\")\n",
    "for play, text in files.items():\n",
    "    corpus_chunks[play] = [Chunk(t, play, None) for t in semchunk.chunk(text, chunk_size = 64, token_counter=lambda text: len(encoder.encode(text)))]\n",
    "\n",
    "print(\"Embedding chunks...\")\n",
    "for play in doc_names:\n",
    "    for chunk in corpus_chunks[play]:\n",
    "        chunk.embedding = model.encode(chunk.text)\n",
    "\n",
    "print(\"Complete\")\n",
    "\n",
    "for play in doc_names:\n",
    "    print(f'{play}: {len(corpus_chunks[play])} chunks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17442\n",
      "17442\n"
     ]
    }
   ],
   "source": [
    "# Combine all play's chunks into a single list\n",
    "chunks = [chunk for play in doc_names for chunk in corpus_chunks[play]]\n",
    "print(sum([len(corpus_chunks[play]) for play in doc_names]))\n",
    "print(len(chunks))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create functions to search through those chunked plays for relevant sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cosine_similarity (vector1, vector2):\n",
    "    dot_product = np.dot(vector1, vector2)\n",
    "    norm_vector1 = np.linalg.norm(vector1)\n",
    "    norm_vector2 = np.linalg.norm(vector2)\n",
    "    return dot_product / (norm_vector1 * norm_vector2)\n",
    "\n",
    "def check_document_relevance(query):\n",
    "    class Relevance:\n",
    "        def __init__(self, name, snippet, similarity):\n",
    "            self.name = name\n",
    "            self.snippet = snippet\n",
    "            self.similarity = similarity\n",
    "\n",
    "    query_embedding = model.encode(query)\n",
    "\n",
    "    print(f'Relevance for \"{query}\":')\n",
    "    print('--------------')\n",
    "\n",
    "    relevances = []\n",
    "    for chunk in chunks:\n",
    "        similarity = cosine_similarity(query_embedding, chunk.embedding)\n",
    "        relevances.append(Relevance(chunk.doc_name, chunk.text, similarity))\n",
    "    relevances.sort(key=lambda x: x.similarity, reverse=True)\n",
    "    for answer in relevances[:5]:\n",
    "        print(f'{answer.name} ({answer.similarity}): {answer.snippet[:500]}\\n')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relevance for \"What was Romeo's last name?\":\n",
      "--------------\n",
      "Romeo and Juliet (0.7419300675392151): Enter ROMEO\n",
      "\n",
      "Romeo and Juliet (0.6840879917144775): JULIET\n",
      "O Romeo, Romeo! wherefore art thou Romeo?\n",
      "Deny thy father and refuse thy name;\n",
      "Or, if thou wilt not, be but sworn my love,\n",
      "And I'll no longer be a Capulet.\n",
      "ROMEO\n",
      "\n",
      "Romeo and Juliet (0.6790132522583008): ROMEO\n",
      "Wilt thou provoke me? then have at thee, boy!\n",
      "They fight\n",
      "\n",
      "Romeo and Juliet (0.6586402654647827): Why dost thou stay?\n",
      "Exit ROMEO\n",
      "\n",
      "Romeo and Juliet (0.6539598107337952): Not Romeo, prince, he was Mercutio's friend;\n",
      "His fault concludes but what the law should end,\n",
      "The life of Tybalt.\n",
      "PRINCE\n",
      "And for that offence\n",
      "Immediately we do exile him hence:\n",
      "I have an interest in your hate's proceeding,\n",
      "\n",
      "\n",
      "Relevance for \"Who stabbed Julius Caesar?\":\n",
      "--------------\n",
      "The Life and Death of Julius Caesar (0.7105638384819031): Caesar, thou art revenged,\n",
      "Even with the sword that kill'd thee.\n",
      "Dies\n",
      "\n",
      "The Life and Death of Julius Caesar (0.6773259043693542): Did not great Julius bleed for justice' sake?\n",
      "What villain touch'd his body, that did stab,\n",
      "And not for justice? What, shall one of us\n",
      "That struck the foremost man of all this world\n",
      "But for supporting robbers, shall we now\n",
      "Contaminate our fingers with base bribes,\n",
      "\n",
      "The Life and Death of Julius Caesar (0.6682009696960449): Caesar must bleed for it! And, gentle friends,\n",
      "Let's kill him boldly, but not wrathfully;\n",
      "Let's carve him as a dish fit for the gods,\n",
      "Not hew him as a carcass fit for hounds:\n",
      "And let our hearts, as subtle masters do,\n",
      "\n",
      "The Life and Death of Julius Caesar (0.6460619568824768): Great Caesar,--\n",
      "CAESAR\n",
      "Doth not Brutus bootless kneel?\n",
      "CASCA\n",
      "Speak, hands for me!\n",
      "CASCA first, then the other Conspirators and BRUTUS stab CAESAR\n",
      "\n",
      "Antony and Cleopatra (0.6407427787780762): DOLABELLA\n",
      "O sir, you are too sure an augurer;\n",
      "That you did fear is done.\n",
      "OCTAVIUS CAESAR\n",
      "Bravest at the last,\n",
      "She levell'd at our purposes, and, being royal,\n",
      "Took her own way. The manner of their deaths?\n",
      "\n",
      "\n",
      "CPU times: total: 1.08 s\n",
      "Wall time: 462 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "check_document_relevance(\"What was Romeo's last name?\")\n",
    "check_document_relevance(\"Who stabbed Julius Caesar?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a mere 17k vectors but it takes hundreds of milliseconds to search for two query strings. This begins to show the challenge of using cosine similarity directly (which scales linearly) and the need for a more efficient algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FAISS requires an index. There are many ways of indexing vectors, this notebook will explore some of the simpler ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flat L2 index\n",
    "\n",
    "Indexes based on the distance between vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "17442\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy\n",
    "\n",
    "# Create the index\n",
    "index = faiss.IndexFlatL2(len(chunks[0].embedding))\n",
    "\n",
    "# Flat L2 index doesn't need trained since it is not specific to the data set\n",
    "print(index.is_trained)\n",
    "\n",
    "# Add embeddings\n",
    "index.add(numpy.array([c.embedding for c in chunks]))\n",
    "print(index.ntotal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, as before, we can search but we'll use `index.search` instead of cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_document_relevance_faiss(query):\n",
    "    class Relevance:\n",
    "        def __init__(self, name, snippet, similarity):\n",
    "            self.name = name\n",
    "            self.snippet = snippet\n",
    "            self.similarity = similarity\n",
    "\n",
    "    query_embedding = model.encode([query])\n",
    "\n",
    "    print(f'Relevance for \"{query}\":')\n",
    "    print('--------------')\n",
    "\n",
    "    distances, i = index.search(query_embedding, 5)\n",
    "    \n",
    "\n",
    "    relevances = []\n",
    "    for chunk in chunks:\n",
    "\n",
    "        relevances.append(Relevance(chunk.doc_name, chunk.text, similarity))\n",
    "    relevances.sort(key=lambda x: x.similarity, reverse=True)\n",
    "    for answer in relevances[:5]:\n",
    "        print(f'{answer.name} ({answer.similarity}): {answer.snippet[:500]}\\n')\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

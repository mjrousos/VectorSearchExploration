{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Facebook AI Similarity Search\n",
    "\n",
    "Comparing two vectors is easy. Cosine similarity is a simple and effective technique. But what if you have thousands or even millions of vectors? How do you find the most similar vectors to a given query vector? Time required to compare (via cosine similarity) with all vectors scales linearly, so comparing to a hundred vectors is not a problem but even our simple searches through plays in the previous workbook took ~200ms to run. On a much larger corpus this would take seconds or tens of seconds for a search. This is the problem that similarity search solves. Running cosine similarity over such a large set of vectors is not feasible, so we need to use other approaches. This is what [Facebook AI Similarity Search (FAISS)](https://ai.meta.com/tools/faiss/) does. It uses indexing to speed up the search process.\n",
    "\n",
    "Available indexes are documented at https://github.com/facebookresearch/faiss/wiki/Faiss-indexes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Available models: https://www.sbert.net/docs/pretrained_models.html\n",
    "model = SentenceTransformer('multi-qa-mpnet-base-dot-v1')\n",
    "# model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "# model = SentenceTransformer('multi-qa-distilbert-cos-v1')\n",
    "# model = SentenceTransformer('sentence-transformers/all-roberta-large-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corpus of documents to search\n",
    "import os\n",
    "\n",
    "directory = \".\\plays\"\n",
    "files = {}\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        with open(file_path, \"r\") as file:\n",
    "            file_name = os.path.splitext(filename)[0]\n",
    "            file_contents = file.read()\n",
    "            files[file_name] = file_contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For FAISS exploration, let's use a small chunk size so that we get many vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import semchunk\n",
    "import tiktoken\n",
    "\n",
    "class Chunk:\n",
    "    def __init__(self, text, doc_name, embedding):\n",
    "        self.text = text\n",
    "        self.doc_name = doc_name\n",
    "        self.embedding = embedding\n",
    "\n",
    "# Chunk the files into smaller portions\n",
    "corpus_chunks = {}\n",
    "doc_names = list(files.keys())\n",
    "encoder = tiktoken.encoding_for_model('gpt-4')\n",
    "\n",
    "print(\"Chunking plays...\")\n",
    "for play, text in files.items():\n",
    "    corpus_chunks[play] = [Chunk(t, play, None) for t in semchunk.chunk(text, chunk_size = 64, token_counter=lambda text: len(encoder.encode(text)))]\n",
    "\n",
    "print(\"Embedding chunks...\")\n",
    "for play in doc_names:\n",
    "    for chunk in corpus_chunks[play]:\n",
    "        chunk.embedding = model.encode(chunk.text)\n",
    "\n",
    "print(\"Complete\")\n",
    "\n",
    "for play in doc_names:\n",
    "    print(f'{play}: {len(corpus_chunks[play])} chunks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all play's chunks into a single list\n",
    "chunks = [chunk for play in doc_names for chunk in corpus_chunks[play]]\n",
    "print(sum([len(corpus_chunks[play]) for play in doc_names]))\n",
    "print(len(chunks))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create functions to search through those chunked plays for relevant sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cosine_similarity (vector1, vector2):\n",
    "    dot_product = np.dot(vector1, vector2)\n",
    "    norm_vector1 = np.linalg.norm(vector1)\n",
    "    norm_vector2 = np.linalg.norm(vector2)\n",
    "    return dot_product / (norm_vector1 * norm_vector2)\n",
    "\n",
    "def check_document_relevance(query):\n",
    "    class Relevance:\n",
    "        def __init__(self, name, snippet, similarity):\n",
    "            self.name = name\n",
    "            self.snippet = snippet\n",
    "            self.similarity = similarity\n",
    "\n",
    "    query_embedding = model.encode(query)\n",
    "\n",
    "    print(f'Relevance for \"{query}\":')\n",
    "    print('--------------')\n",
    "\n",
    "    relevances = []\n",
    "    for chunk in chunks:\n",
    "        similarity = cosine_similarity(query_embedding, chunk.embedding)\n",
    "        relevances.append(Relevance(chunk.doc_name, chunk.text, similarity))\n",
    "    relevances.sort(key=lambda x: x.similarity, reverse=True)\n",
    "    for answer in relevances[:5]:\n",
    "        print(f'{answer.name} ({answer.similarity}): {answer.snippet[:500]}\\n')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "check_document_relevance(\"What was Romeo's last name?\")\n",
    "check_document_relevance(\"Who stabbed Julius Caesar?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a mere 17k vectors but it takes hundreds of milliseconds to search for two query strings. This begins to show the challenge of using cosine similarity directly (which scales linearly) and the need for a more efficient algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FAISS requires an index. There are many ways of indexing vectors, this notebook will explore some of the simpler ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flat L2 index\n",
    "\n",
    "Indexes based on the distance between vectors. This will give the highest quality matches but is slower than other FAISS indeces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy\n",
    "\n",
    "# Create the index\n",
    "index = faiss.IndexFlatL2(len(chunks[0].embedding))\n",
    "\n",
    "# Flat L2 index doesn't need trained since it is not specific to the data set\n",
    "print(index.is_trained)\n",
    "\n",
    "# Add embeddings\n",
    "index.add(numpy.array([c.embedding for c in chunks]))\n",
    "print(index.ntotal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, as before, we can search but we'll use `index.search` instead of cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_document_relevance_faiss(query, index):\n",
    "    class Relevance:\n",
    "        def __init__(self, name, snippet, similarity):\n",
    "            self.name = name\n",
    "            self.snippet = snippet\n",
    "            self.similarity = similarity\n",
    "\n",
    "    query_embedding = model.encode([query])\n",
    "\n",
    "    print(f'Relevance for \"{query}\":')\n",
    "    print('--------------')\n",
    "\n",
    "    # Searching returns both the distances of each vector from the query vector\n",
    "    # and the indeces for those nearest neighbors\n",
    "    distances, indeces = index.search(query_embedding, 5)\n",
    "    \n",
    "    # Note that relevance with FAISS is different from cosine similarity.\n",
    "    # Now *lower* values are more relevant instead of higher ones.\n",
    "    relevances = [Relevance(chunks[i].doc_name, chunks[i].text, distances[0][j]) for j, i in enumerate(indeces[0])]\n",
    "\n",
    "    for answer in relevances[:5]:\n",
    "        print(f'{answer.name} ({answer.similarity}): {answer.snippet[:500]}\\n')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "check_document_relevance_faiss(\"What was Romeo's last name?\", index)\n",
    "check_document_relevance_faiss(\"Who stabbed Julius Caesar?\", index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much better. But it can still be improved! While using a FAISS index to search improves performance over just running cosine similarity over every vector, it still scales linearly and will run into performance problems as the set of vectors we're testing against grows. Let's look at other FAISS index options that can help."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flat IP\n",
    "\n",
    "This is very similar to FlatL2 except that it uses a slightly different algorithm for determining distance between vectors. Flat L2 looks at distance between vectors whereas Flat IP is the inner product (dot product) of the two. Flat IP is like cosine similarity except that it's not normalized (so IP cares about vector magnitude whereas cosine similarity does not)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flat IVF Index\n",
    "\n",
    "To avoid the problem of linearly scaling search times with a flat index, we can partition vectors into groups (\"cells\") based on their position in vector space. Each group is assigned a centroid which is a centrally-located element in the cell. Now, when we search, FAISS can first identify the closest centroid to the query vector using a flat index approach and then search only that partition for results. This is referred to as an [inverted file index](https://github.com/facebookresearch/faiss/wiki/Faiss-indexes#cell-probe-methods-indexivf-indexes) because the cells are inverted lists.\n",
    "\n",
    "In some scenarios it might also make sense to identify a small number of centroids closest to the query vector, instead, to account for scenarios where the query vector is near a \"boundary\" of a partition.\n",
    "\n",
    "Note that this is an *approximate* search (as opposed to flat L2) since we're not checking all possible matches and could in some cases miss relevant ones. It is, however, much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cell_count = 10\n",
    "\n",
    "# Index IVF requires another trained index to use for comparison within clusters and a count\n",
    "# for the number of clusters\n",
    "indexIVF = faiss.IndexIVFFlat(index, len(chunks[0].embedding), cell_count)\n",
    "\n",
    "# IVF indeces require training to assign the centroids.\n",
    "print(\"IVF trained initially: \", indexIVF.is_trained)\n",
    "indexIVF.train(numpy.array([c.embedding for c in chunks]))\n",
    "print(\"IVF trained after training: \", indexIVF.is_trained)\n",
    "indexIVF.add(numpy.array([c.embedding for c in chunks]))\n",
    "print(indexIVF.ntotal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "check_document_relevance_faiss(\"What was Romeo's last name?\", indexIVF)\n",
    "check_document_relevance_faiss(\"Who stabbed Julius Caesar?\", indexIVF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The IVF index's nprobe value determines how many centroids to search. A higher nprobe value will search more centroids and return more accurate results but will be slower. A lower nprobe value will be faster but may miss relevant results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexIVF.nprobe = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "check_document_relevance_faiss(\"What was Romeo's last name?\", indexIVF)\n",
    "check_document_relevance_faiss(\"Who stabbed Julius Caesar?\", indexIVF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Product quantization\n",
    "\n",
    "Another tweak on IVF index is IVF with product quantization. This is a more advanced technique that partitions vectors into sub-vectors, each of which is mapped to a centroid for that portion of the vector. These centroid IDs are then stored in the index. This allows for more efficient storage and search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the number of segments to split the vectors into\n",
    "# The number must evenly divide the vector length\n",
    "m = 8\n",
    "\n",
    "# Bits in the centroid\n",
    "bits = 8\n",
    "\n",
    "cell_count = 10\n",
    "\n",
    "indexIVFPQ = faiss.IndexIVFPQ(index, len(chunks[0].embedding), cell_count, m, bits)\n",
    "indexIVFPQ.train(numpy.array([c.embedding for c in chunks]))\n",
    "print(indexIVFPQ.is_trained)\n",
    "indexIVFPQ.add(numpy.array([c.embedding for c in chunks]))\n",
    "print(indexIVFPQ.ntotal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "check_document_relevance_faiss(\"What was Romeo's last name?\", indexIVFPQ)\n",
    "check_document_relevance_faiss(\"Who stabbed Julius Caesar?\", indexIVFPQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance comparer\n",
    "def compare_performance(test_index, query_embedding, search_count):\n",
    "    distances, indeces = test_index.search(query_embedding, search_count)\n",
    "    print(indeces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_embeddings = model.encode([\"What was Romeo's last name?\", \"Who stabbed Julius Caesar?\", \"Where is Hamlet set?\", \"Who is Oberon?\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "compare_performance(index, test_embeddings, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "compare_performance(indexIVF, test_embeddings, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "compare_performance(indexIVFPQ, test_embeddings, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Times are all quite close and quite small, but it is clear that the IVF and IVFPQ indeces are faster than the flat L2 index. Note that the results do vary with the flat L2 index being the most accurate and the others being similar but not quite as good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Locality Sensitive Hashing (LSH)\n",
    "\n",
    "Works by hashing vectors into buckets. Unlike other common hashing scenarios, LSH does not try to minimize collisions. Instead, it hashes in such a way that similar vectors are *likely* to collide. Consequently, similar vectors are more likely to be hashed into the same bucket. This is a very fast way to search for similar vectors but is approximate and can miss relevant results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = len(chunks[0].embedding)\n",
    "\n",
    "# Number of bits to use for the hash\n",
    "# Higher numbers will have higher hash granularity (and, therefore, both higher accuracy and slower speed)\n",
    "# Very high nbits values can even make this slower than Flat L2/Flat IP\n",
    "nbits = int(d / 2)\n",
    "\n",
    "indexLSH = faiss.IndexLSH(d, nbits)\n",
    "indexLSH.add(numpy.array([c.embedding for c in chunks]))\n",
    "\n",
    "print(indexLSH.is_trained)\n",
    "print(indexLSH.ntotal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "compare_performance(indexLSH, test_embeddings, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical Navigable Small World (HNSW)\n",
    "\n",
    "Navigable small world is a (potentially large) graph with a small number of hops between any two vertices.\n",
    "\n",
    "Hierarchical NSW is an NSW split across multiple layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Higher parameters are more accurate, lower are faster\n",
    "connection_count = 16\n",
    "search_depth = 16\n",
    "construction_search_depth = 64 # Slows index creation but not runtime\n",
    "\n",
    "indexHNSW = faiss.IndexHNSWFlat(d, connection_count)\n",
    "\n",
    "indexHNSW.hnsw.efSearch = search_depth\n",
    "indexHNSW.hnsw.efConstruction = construction_search_depth\n",
    "\n",
    "print(indexHNSW.is_trained)\n",
    "\n",
    "indexHNSW.add(numpy.array([c.embedding for c in chunks]))\n",
    "\n",
    "print (indexHNSW.ntotal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "compare_performance(indexHNSW, test_embeddings, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance comparison\n",
    "\n",
    "Let's compare performance of some of these algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def measure_time(name, iterations, func, *args, **kwargs):\n",
    "    start_time = time.time()\n",
    "    for i in range(iterations):\n",
    "        func(*args, **kwargs)\n",
    "    end_time = time.time()\n",
    "    print(f\"{name}: {end_time - start_time} seconds\")\n",
    "    return end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algorithms = {\n",
    "    'Flat L2': index,\n",
    "    'Flat IVF': indexIVF,\n",
    "    'IVF PQ': indexIVFPQ,\n",
    "    'LSH': indexLSH,\n",
    "    'HNSW': indexHNSW\n",
    "}\n",
    "\n",
    "timings = [(name, measure_time(name, 1000, lambda: test_index.search(test_embeddings, 5))) for name, test_index in algorithms.items()]\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "keys, values = zip(*timings)\n",
    "plt.bar(keys, values)\n",
    "plt.xlabel('Algorithm')\n",
    "plt.ylabel('Timing (x1000)')\n",
    "plt.title('Algorithm Timing')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

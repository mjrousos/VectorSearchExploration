{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Facebook AI Similarity Search\n",
    "\n",
    "Comparing two vectors is easy. Cosine similarity is a simple and effective technique. But what if you have thousands or even millions of vectors? How do you find the most similar vectors to a given query vector? Time required to compare (via cosine similarity) with all vectors scales linearly, so comparing to a hundred vectors is not a problem but even our simple searches through plays in the previous workbook took ~200ms to run. On a much larger corpus this would take seconds or tens of seconds for a search. This is the problem that similarity search solves. Running cosine similarity over such a large set of vectors is not feasible, so we need to use other approaches. This is what [Facebook AI Similarity Search (FAISS)](https://ai.meta.com/tools/faiss/) does. It uses indexing to speed up the search process.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Available models: https://www.sbert.net/docs/pretrained_models.html\n",
    "model = SentenceTransformer('multi-qa-mpnet-base-dot-v1')\n",
    "# model = SentenceTransformer('bert-base-nli-mean-tokens')\n",
    "# model = SentenceTransformer('multi-qa-distilbert-cos-v1')\n",
    "# model = SentenceTransformer('sentence-transformers/all-roberta-large-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corpus of documents to search\n",
    "import os\n",
    "\n",
    "directory = \".\\plays\"\n",
    "files = {}\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        with open(file_path, \"r\") as file:\n",
    "            file_name = os.path.splitext(filename)[0]\n",
    "            file_contents = file.read()\n",
    "            files[file_name] = file_contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For FAISS exploration, let's use a small chunk size so that we get many vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import semchunk\n",
    "import tiktoken\n",
    "\n",
    "class Chunk:\n",
    "    def __init__(self, text, doc_name, embedding):\n",
    "        self.text = text\n",
    "        self.doc_name = doc_name\n",
    "        self.embedding = embedding\n",
    "\n",
    "# Chunk the files into smaller portions\n",
    "corpus_chunks = {}\n",
    "doc_names = list(files.keys())\n",
    "encoder = tiktoken.encoding_for_model('gpt-4')\n",
    "\n",
    "print(\"Chunking plays...\")\n",
    "for play, text in files.items():\n",
    "    corpus_chunks[play] = [Chunk(t, play, None) for t in semchunk.chunk(text, chunk_size = 64, token_counter=lambda text: len(encoder.encode(text)))]\n",
    "\n",
    "print(\"Embedding chunks...\")\n",
    "for play in doc_names:\n",
    "    for chunk in corpus_chunks[play]:\n",
    "        chunk.embedding = model.encode(chunk.text)\n",
    "\n",
    "print(\"Complete\")\n",
    "\n",
    "for play in doc_names:\n",
    "    print(f'{play}: {len(corpus_chunks[play])} chunks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all play's chunks into a single list\n",
    "chunks = [chunk for play in doc_names for chunk in corpus_chunks[play]]\n",
    "print(sum([len(corpus_chunks[play]) for play in doc_names]))\n",
    "print(len(chunks))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create functions to search through those chunked plays for relevant sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cosine_similarity (vector1, vector2):\n",
    "    dot_product = np.dot(vector1, vector2)\n",
    "    norm_vector1 = np.linalg.norm(vector1)\n",
    "    norm_vector2 = np.linalg.norm(vector2)\n",
    "    return dot_product / (norm_vector1 * norm_vector2)\n",
    "\n",
    "def check_document_relevance(query):\n",
    "    class Relevance:\n",
    "        def __init__(self, name, snippet, similarity):\n",
    "            self.name = name\n",
    "            self.snippet = snippet\n",
    "            self.similarity = similarity\n",
    "\n",
    "    query_embedding = model.encode(query)\n",
    "\n",
    "    print(f'Relevance for \"{query}\":')\n",
    "    print('--------------')\n",
    "\n",
    "    relevances = []\n",
    "    for chunk in chunks:\n",
    "        similarity = cosine_similarity(query_embedding, chunk.embedding)\n",
    "        relevances.append(Relevance(chunk.doc_name, chunk.text, similarity))\n",
    "    relevances.sort(key=lambda x: x.similarity, reverse=True)\n",
    "    for answer in relevances[:5]:\n",
    "        print(f'{answer.name} ({answer.similarity}): {answer.snippet[:500]}\\n')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "check_document_relevance(\"What was Romeo's last name?\")\n",
    "check_document_relevance(\"Who stabbed Julius Caesar?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a mere 17k vectors but it takes hundreds of milliseconds to search for two query strings. This begins to show the challenge of using cosine similarity directly (which scales linearly) and the need for a more efficient algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FAISS requires an index. There are many ways of indexing vectors, this notebook will explore some of the simpler ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flat L2 index\n",
    "\n",
    "Indexes based on the distance between vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy\n",
    "\n",
    "# Create the index\n",
    "index = faiss.IndexFlatL2(len(chunks[0].embedding))\n",
    "\n",
    "# Flat L2 index doesn't need trained since it is not specific to the data set\n",
    "print(index.is_trained)\n",
    "\n",
    "# Add embeddings\n",
    "index.add(numpy.array([c.embedding for c in chunks]))\n",
    "print(index.ntotal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, as before, we can search but we'll use `index.search` instead of cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_document_relevance_faiss(query):\n",
    "    class Relevance:\n",
    "        def __init__(self, name, snippet, similarity):\n",
    "            self.name = name\n",
    "            self.snippet = snippet\n",
    "            self.similarity = similarity\n",
    "\n",
    "    query_embedding = model.encode([query])\n",
    "\n",
    "    print(f'Relevance for \"{query}\":')\n",
    "    print('--------------')\n",
    "\n",
    "    # Searching returns both the distances of each vector from the query vector\n",
    "    # and the indeces for those nearest neigbors\n",
    "    distances, indeces = index.search(query_embedding, 5)\n",
    "    \n",
    "    # Note that relevance with FAISS is different from cosine similarity.\n",
    "    # Now *lower* values are more relevant instead of higher ones.\n",
    "    relevances = [Relevance(chunks[i].doc_name, chunks[i].text, distances[0][j]) for j, i in enumerate(indeces[0])]\n",
    "\n",
    "    for answer in relevances[:5]:\n",
    "        print(f'{answer.name} ({answer.similarity}): {answer.snippet[:500]}\\n')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "check_document_relevance_faiss(\"What was Romeo's last name?\")\n",
    "check_document_relevance_faiss(\"Who stabbed Julius Caesar?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much better. But it can still be improved!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
